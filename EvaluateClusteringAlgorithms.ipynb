{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering_algorithm(algorithm, X, y, eval_algo):\n",
    "    import time\n",
    "    import resource\n",
    "    t1 = time.time()\n",
    "    predicted_clusters = algorithm(X)\n",
    "    exec_time = time.time() - t1\n",
    "    import resource\n",
    "    mem_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    predicted_clusters = map_their_labels_to_ours(predicted_clusters, y)\n",
    "    performance = eval_algo(predicted_clusters, y)\n",
    "    return exec_time, mem_usage, performance\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_algo(pred, act):\n",
    "    # f1\n",
    "    import sklearn\n",
    "    return sklearn.metrics.f1_score(act, pred, average=\"micro\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomasjarosz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/thomasjarosz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing all the neccesary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data={'bio':pd.read_csv('./biology.csv',index_col=0),\n",
    "      'robo':pd.read_csv('./robotics.csv',index_col=0),\n",
    "      'cooking':pd.read_csv('./cooking.csv',index_col=0)}\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def clean_content(table):\n",
    "    content = table.content\n",
    "    #Converting text to lowercase characters\n",
    "    content = content.apply(lambda x: x.lower())\n",
    "    #Removing HTML tags\n",
    "    content = content.apply(lambda x: re.sub(r'\\<[^<>]*\\>','',x))\n",
    "    #Removing any character which does not match to letter,digit or underscore\n",
    "    content = content.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n",
    "    #Removing space,newline,tab\n",
    "    content = content.apply(lambda x: re.sub(r'\\s',' ',x))\n",
    "    #Removing punctuation\n",
    "    content = content.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "    #Tokenizing data\n",
    "    content = content.apply(lambda x: word_tokenize(x))\n",
    "    #Removing stopwords\n",
    "    content = content.apply(lambda x: [i for i in x if i not in stops])\n",
    "    return(content)\n",
    "   \n",
    "def clean_title(table):\n",
    "    title = table.title\n",
    "    title = title.apply(lambda x: x.lower())\n",
    "    title = title.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n",
    "    title = title.apply(lambda x: re.sub(r'\\s',' ',x))\n",
    "    title = title.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "    title = title.apply(lambda x: word_tokenize(x))\n",
    "    title = title.apply(lambda x: [i for i in x if i not in stops])\n",
    "    return(title)\n",
    "   \n",
    "\n",
    "for df in data:\n",
    "    data[df].content = clean_content(data[df])\n",
    "  \n",
    "for df in data:\n",
    "    data[df].title = clean_title(data[df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_y(data_dict, percentage_of_total = .1):\n",
    "    vect = TfidfVectorizer(tokenizer=identity_tokenizer,lowercase=False)\n",
    "    ret = []\n",
    "    y = np.array([0]* int(len(data_dict[\"bio\"].content) * percentage_of_total))\n",
    "    y = np.append(y, np.array([1]*int(len(data_dict[\"robo\"].content) * percentage_of_total)), axis=0)\n",
    "    y = np.append(y, np.array([2]*int(len(data_dict[\"cooking\"].content) * percentage_of_total)), axis=0)\n",
    "    pre_vectorized_X = data['bio'].content.values[0:int(data['bio'].content.values.shape[0] * percentage_of_total)]\n",
    "    pre_vectorized_X = np.append(pre_vectorized_X, data['robo'].content.values[0:int(data['robo'].content.values.shape[0] * percentage_of_total)])\n",
    "    pre_vectorized_X = np.append(pre_vectorized_X, data['cooking'].content.values[0:int(data['cooking'].content.values.shape[0] * percentage_of_total)])\n",
    "    X = vect.fit_transform(pre_vectorized_X)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_map_their_labels_to_ours(their_labels, our_labels):\n",
    "    from collections import Counter\n",
    "    their_unique_labels = np.unique(their_labels)\n",
    "    their_labels_used = set()\n",
    "    ret = {}\n",
    "    for our_label in np.unique(our_labels):\n",
    "        idxs = np.where(our_labels == our_label)\n",
    "        predicted_for_label_l = their_labels[idxs]\n",
    "        most_common = Counter(predicted_for_label_l).most_common()\n",
    "        for label_count_pair in most_common:\n",
    "            their_label = label_count_pair[0]\n",
    "            if their_label not in their_labels_used:\n",
    "                break\n",
    "        their_labels_used.add(their_label)\n",
    "        ret[their_label] = our_label\n",
    "    return ret\n",
    "\n",
    "def map_their_labels_to_ours(their_labels, our_labels):\n",
    "    label_map = find_map_their_labels_to_ours(their_labels, our_labels) \n",
    "    map_preds = lambda x: label_map[x]\n",
    "    return np.vectorize(map_preds)(their_labels)\n",
    "        \n",
    "def kmeans_algo(X):\n",
    "    from sklearn.cluster import KMeans\n",
    "    model = KMeans(n_clusters = 3)\n",
    "    return model.fit_predict(X)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_X_and_y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5684669017791748, 397291520, 0.30165816326530615)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_res = evaluate_clustering_algorithm(kmeans_algo, X,  y, eval_algo)\n",
    "kmeans_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.60228490829468, 882409472, 0.8737244897959183)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AgglomerativeClustering_algo(X):\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    model = AgglomerativeClustering(n_clusters = 3)\n",
    "    return model.fit_predict(X.toarray())\n",
    "AgglomerativeClustering_algo_res = evaluate_clustering_algorithm(AgglomerativeClustering_algo, X,  y, eval_algo)\n",
    "AgglomerativeClustering_algo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106.34011816978455, 3196514304, 0.4209183673469387)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def birch_algo(X):\n",
    "    from sklearn.cluster import Birch\n",
    "    model = Birch(n_clusters = 3)\n",
    "    return model.fit_predict(X)\n",
    "birch_res = evaluate_clustering_algorithm(birch_algo, X,  y, eval_algo)\n",
    "birch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11622405052185059, 381128704, 0.3565051020408163)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MiniBatchKMeans_algo(X):\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    model = MiniBatchKMeans(n_clusters = 3)\n",
    "    return model.fit_predict(X)\n",
    "MiniBatchKMeans_algo_res = evaluate_clustering_algorithm(MiniBatchKMeans_algo, X,  y, eval_algo)\n",
    "MiniBatchKMeans_algo_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.283025741577148, 777080832, 0.7088647959183673)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SpectralClustering_algo(X):\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    model = SpectralClustering(n_clusters = 3)\n",
    "    return model.fit_predict(X)\n",
    "SpectralClustering_algo_res = evaluate_clustering_algorithm(SpectralClustering_algo, X,  y, eval_algo)\n",
    "SpectralClustering_algo_res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs598psl] *",
   "language": "python",
   "name": "conda-env-cs598psl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
